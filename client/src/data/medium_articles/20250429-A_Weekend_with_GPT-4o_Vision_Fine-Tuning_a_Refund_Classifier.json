{
  "title": "A Weekend with GPT-4o Vision: Fine-Tuning a Refund Classifier",
  "url": "https://medium.com/ai-unscripted/a-weekend-with-gpt-4o-vision-fine-tuning-a-refund-classifier-bdd981394966",
  "author": "Kenji",
  "publish_date": null,
  "read_time": "3 min read",
  "claps": 0,
  "comments": 0,
  "cover_image_url": "https://miro.medium.com/v2/resize:fit:875/0*kdiIHiw-EB5njP_i",
  "content": "Member-only story\n\nA Weekend with GPT-4o Vision: Fine-Tuning a Refund Classifier\n\nFrom a folder of “maybe” images to a model that replies with one word — no custom CNN required\n\nCan we spare the team from scrolling through every photo and ticking a box?\n\nCan we spare the team from scrolling through every photo and ticking a box?\n\nThat was the ask. Last week, I picked up a project involving refund requests: customers send pictures, an operations crew reviews each one, and if the product meets a checklist, the money is refunded to the buyer. Simple on paper, tedious in real life.\n\nRather than train a network from the ground up, I'd like to know: Could GPT-4o Vision handle the heavy lifting if I nudged it with the right prompt and a small fine-tune? Short answer — yes, with a couple of quirks. (I had my fingers crossed for GPT-4.1 fine-tuning, but that switch isn’t flipped yet.)\n\nBelow is the code I ran, along with the thought process behind each block, and where I plan to focus my following improvements. Feel free to swap in your folder of “maybe” images and see what happens.\n\n1. Prep the data — Walk the folders and turn paths + labels into one DataFrame\n\nrefund-img/\n├─ claimed/\n└─ unclaimed/\n\ndef build_df(root: Path) -> pd.DataFrame:\n    records = []\n    for lbl in [\"claimed\", \"unclaimed\"]:\n        for r, _, fs in os.walk(root / lbl):\n            for fn in fs:\n                if fn.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n                    records.append(\n                        {\"path\": str(Path(r) / fn), \"label\": lbl}\n                    )\n    return pd.DataFrame(records)\n\ndf = build_df(Path(\"refund-img\"))\nprint(df[\"label\"].value_counts())  # sanity check\n\n2. Split the data — Create train, validation, and test with identical class ratios\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_df, tmp_df = train_test_split(\n    df, test_size=0.30, stratify=df[\"label\"], random_state=142\n)\nval_df,  test_df = train_test_split(\n    tmp_df, test_size=1/3, stratify=tmp_df[\"label\"], random_state=142\n)\n\n3. Ensure that every image is Base-64 Encoded — embed each photo as a data URI so the fine-tune file is self-contained.\n\nimport base64\nfrom tqdm import tqdm\n\ndef inline_base64(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    encoded = []\n    for p in tqdm(df[\"path\"], desc=\"Encoding\"):\n        with open(p, \"rb\") as f:\n            b64 = base64.b64encode(f.read()).decode()\n        mime = \"image/png\" if p.lower().endswith(\".png\") else \"image/jpeg\"\n        encoded.append(f\"data:{mime};base64,{b64}\")\n    df[\"base64_uri\"] = encoded\n    return df\n\ntrain_df = inline_base64(train_df)\nval_df   = inline_base64(val_df)\n\n4. System Prompt (one line does the job):\n\nYou are an assistant that decides whether an item can be returned for a refund.\nAnswer with exactly one word: “claimable” or “non-claimable”.\n\nConversation builder — three turns per example:\n\ndef row_to_chat(r):\n    return {\n        \"messages\": [\n            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n            {\"role\": \"user\",\n             \"content\": [\n                 {\"type\": \"text\", \"text\": \"Is this item claimable?\"},\n                 {\"type\": \"image_url\",\n                  \"image_url\": {\"url\": r[\"base64_uri\"], \"detail\": \"low\"}}\n             ]},\n            {\"role\": \"assistant\",\n             \"content\": [{\"type\": \"text\", \"text\": r[\"label\"]}]}\n        ]\n    }\n\nDump each DataFrame to JSONL — train.jsonl, val.jsonl; keep test.jsonl local for evaluation.\n\n5. Fine-tune GPT-4o Vision — Upload the JSONL files and launch the job.\n\n# 4 a Upload\nopenai files upload --purpose fine-tune --file train.jsonl\nopenai files upload --purpose fine-tune --file val.jsonl\n\n# 4 b Start the run\nopenai fine_tuning.jobs.create \\\n  -t file-TRAIN_ID -v file-VAL_ID \\\n  -m gpt-4o-vision-preview \\\n  -n refund-detector-v0\n\n# 4 c Watch logs\nopenai fine_tuning.jobs.follow -i ftjob-XYZ\n\nAbout half an hour later the dashboard lit up green — “Succeeded.” On the unseen validation photos it was wrong only a fraction of a percent (validation-loss ≈ 0.0008), a reassuring result for a first pass and plenty good enough to take manual triage off the team’s plate.\n\nMy thought\n\nI used to assume image classification meant custom layers and long training cycles. GPT-4o Vision plus a tidy JSONL made that feel like overkill. Clean metadata, a short prompt, two API calls — sometimes that really is enough.\n\nIf you try the structure on your own refund photos (or any binary vision task), I’d love to hear how it lands.",
  "scraped_at": "2025-04-29 10:59:13"
}