{
  "title": "Extracting Parsed Documents from LlamaCloud",
  "url": "https://medium.com/@kenji-onisuka/extracting-parsed-documents-from-llamacloud-072e199bcf6f",
  "author": "Kenji",
  "publish_date": "Jul 17, 2024",
  "read_time": "3 min read",
  "claps": 83,
  "comments": 0,
  "cover_image_url": "https://miro.medium.com/v2/resize:fit:700/1*Nm4VmJ7WHoi_p5Tj1eq8xA.png",
  "content": "Darrin Atkins\n highlighted\n\nDarrin Atkins\n highlighted\n\nExtracting Parsed Documents from LlamaCloud\n\nKenji\n\n83\n\nFor a personal reason which I will not get into, I needed to retrieve already parsed documents from my LlamaCloud account but encountered an unexpected challenge. Despite LlamaCloud’s robust capabilities, I couldn’t find a straightforward method to accomplish this seemingly simple task. This experience led me to develop a custom solution, which I’d like to share as part of my learning journey in software development.\n\nThe Challenge\n\nWhen I needed to retrieve parsed documents from LlamaCloud programmatically, I hit a snag. I couldn’t find a simple way to do this that worked for my needs. There might be an easier solution out there, but since I couldn’t find one, I decided to write a script myself. It was a bit of a learning curve, but it helped me solve my problem. The main function I came up with looks like this:\n\ndef save_parsed_documents(documents, output_dir, file_name):\n    \"\"\"\n    Save each parsed document as a separate markdown file.\n\n    Args:\n    documents (list or str): List of Document objects or a single string\n    output_dir (str): Directory to save the markdown files\n    file_name (str): Original file name to use in the filename\n    \"\"\"\n    Path(output_dir).mkdir(parents=True, exist_ok=True)\n\n    if isinstance(documents, str):\n        output_filename = f\"{Path(file_name).stem}_parsed.md\"\n        output_path = os.path.join(output_dir, output_filename)\n\n        with open(output_path, 'w', encoding='utf-8') as f:\n            f.write(documents)\n\n        logger.info(f\"Saved parsed content to: {output_path}\")\n    else:\n        for idx, doc in enumerate(documents, start=1):\n            if not doc.metadata or 'file_path' not in doc.metadata or not doc.text:\n                logger.warning(\"Document metadata or text missing, skipping document.\")\n                continue\n\n            original_filename = Path(doc.metadata['file_path']).stem\n            output_filename = f\"{original_filename}_parsed.md\"\n            output_path = os.path.join(output_dir, output_filename)\n\n            with open(output_path, 'w', encoding='utf-8') as f:\n                f.write(f\"# Parsed content from: {doc.metadata.get('file_name', 'Unknown')}\\n\\n\")\n                f.write(doc.text)\n\n            logger.info(f\"Saved parsed content to: {output_path}\")\n\ndef get_parsing_history(api_token):\n    url = \"https://cloud.llamaindex.ai/api/v1/parsing/history\"\n    headers = {\n        'Accept': 'application/json',\n        'Authorization': f'Bearer {api_token}'\n    }\n    try:\n        response = requests.get(url, headers=headers)\n        response.raise_for_status()\n        return response.json()\n    except requests.exceptions.HTTPError as http_err:\n        if response.status_code == 404:\n            logger.error(f\"404 Error: The endpoint '{url}' was not found. Please check the URL.\")\n        else:\n            logger.error(f\"HTTP error occurred: {http_err}\")\n    except requests.exceptions.RequestException as req_err:\n        logger.error(f\"Request error occurred: {req_err}\")\n    return None\n\ndef filter_active_jobs(parsing_history, days=3):\n    active_jobs = []\n    cutoff_date = datetime.now() - timedelta(days=days)\n    for entry in parsing_history:\n        job_date = datetime.strptime(entry['day'], \"%Y-%m-%d\")\n        if job_date >= cutoff_date and not entry['expired']:\n            active_jobs.append(entry)\n    return active_jobs\n\nasync def retrieve_parsed_document(api_key, job_id, output_dir, original_file_name):\n    try:\n        parser = LlamaParse(api_key=api_key)\n\n        # Using the `_get_job_result` method to retrieve documents\n        result = await parser._get_job_result(job_id=job_id, result_type=\"markdown\")\n\n        if isinstance(result, dict) and \"markdown\" in result:\n            documents = result[\"markdown\"]\n        else:\n            logger.error(\"Unexpected response format from parser._get_job_result\")\n            return\n\n        save_parsed_documents(documents, output_dir, original_file_name)\n    except Exception as e:\n        logger.error(f\"Failed to retrieve parsed document for job ID {job_id}: {e}\")\n\nasync def main():\n    output_dir = os.getenv('OUTPUT_DIR', r'D:\\#Generative AI\\Retrieval Augmented Generation (RAG)\\01-Use-Case-Call-Center-Agent\\demo-data\\_retrieved_docs')\n\n    parsing_history = get_parsing_history(api_token_llamacloud)\n    if not parsing_history:\n        logger.info(\"No parsing history found.\")\n        return\n\n    active_jobs = filter_active_jobs(parsing_history, days=3)\n    if not active_jobs:\n        logger.info(\"No active jobs found in the last 3 days.\")\n        return\n\n    tasks = []\n    for job in active_jobs:\n        job_id = job['job_id']\n        original_file_name = job['original_file_name']\n        logger.info(f\"Retrieving document with Job ID: {job_id}\")\n        tasks.append(retrieve_parsed_document(api_key_llamaparse, job_id, output_dir, original_file_name))\n\n    await asyncio.gather(*tasks)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\nConclusion\n\nWhile my script may not be the most elegant or official solution for retrieving parsed documents from LlamaCloud, it solved my specific problem and enhanced my understanding of both the platform and asynchronous Python programming.\n\nI’m sharing this experience not as a definitive guide to working with LlamaCloud, but as a snapshot of my journey. This solution worked for me, but it’s important to note that it might not be the best or only way to accomplish this task. If you know of a simpler method, I’d be genuinely interested in learning about it!",
  "scraped_at": "2025-04-07 09:06:04"
}